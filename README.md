# Problem Statement
Test the following hypothesis:
"The energy distribution of the single particle showers produced by CWGAN-GP is statistically indistinguishable from that of Geant4, meaning the generated data accurately represents Geant4 showers"

# Why is it important?
Simulation is essential requirement for comparing with actual data from CERN for analysis of high energy experiments. Traditionally simulation is done by Geant4, a resource intensive tool.
This model conditional WGAN-gradient penalty created at University of Bologna is a fast, lightweight alternate to traditional resource intensive simulation tool (Geant4) for the particle physics ATLAS experiment at CERN in Geneva, Switzerland. 

# Goal
To implement hypothesis testing (chi-squared test) between two distributions with the following null hypothesis: Null Hypothesis (H0): “Energy distribution of single-particle pion shower generated by BoloGAN is statistically indistinguishable from that produced by Geant4 Full Simulation.”
One distribution is energy distribution Geant4 from dataset  (csvFiles, rootFiles, binning.xml), the other is distribution is generated by the conditional WGAN-GP model.

## Dataset

A binning file containing radial and angular binning and the hyperparameters. Voxels defined and maximized per layer and eta η
<img width="364" alt="image" src="https://github.com/user-attachments/assets/90a8aeac-3aa4-42f5-8d6c-ec81733d9eb5" />

CSV files containing single particle energy depositions flattened voxels

ROOT files containing original histogram containing Geant4 full normalized energy distribution “h_vox”, the statistics of events in ROOT files for every energy level given:
<img width="863" alt="image" src="https://github.com/user-attachments/assets/2031dc98-5d7d-4fb1-a4a3-3af9d24aaf41" />

ROOT file structure
<img width="252" alt="Screenshot 2025-02-24 123818" src="https://github.com/user-attachments/assets/0974bc35-bf02-4046-8034-297bf58a5dfe" />


## A brief outline of BoloGAN
+ Model: conditional Wasserstein GAN with Gradient Penalty
+ cWGAN-GP model
+ <img width="522" alt="Screenshot 2025-02-18 060956" src="https://github.com/user-attachments/assets/7ad32fbe-d213-438d-9925-ff8e1b1205dc" />
+ Loss function: <img width="135" alt="image" src="https://github.com/user-attachments/assets/bd062795-3394-4f93-8608-7403ed26755e" />
+ Final objective function after gradient penalty <img width="328" alt="image" src="https://github.com/user-attachments/assets/66ec8062-4de5-44df-8ff4-b67850604502" />

## Methodology

+ The goal is to implement hypothesis testing (chi-squared test) between two distributions with the following null hypothesis: Null Hypothesis (H0): “Energy distribution of single-particle pion shower generated by BoloGAN is statistically indistinguishable from that produced by Geant4 Full Simulation.”

+ Generate energy distribution in voxels from the generator and normalize the energy.

+ Plot the normalized energy distribution in GeV (with hits “entries” on the y-axis) as histogram “h_gan” via ROOT.TH1F.

+ Retrieve the original histogram containing Geant4 full normalized energy distribution “h_vox” from the ROOT files for comparison.

+ Evaluate the chi-squared test using chi2test(h_vox, h_gan) to obtain chi² and ndf (number of degrees of freedom).

+ Visualize the results via TensorBoard.
  
+ Pass hyperparameters through binning.xml file to the BoloGAN model

+ Execute Bash scripts to run train.py to define data, user and home directories along with particle id, η range, epoch numbers and the binning.xml file

+ Utilize LXPLUS batch at CERN and conduct runs by submitting jobs to HTCondor pool, using NVIDIA H100/A100/V100 GPUs

+ Conducted preliminary runs for selecting best performing models with best generator architecture

+ Performed systematic random search for optimizing hyperparameters of batch size, learning rate and Discriminator/Generator ratio

+ Trained each model for multiple (minimum 3) runs

+ Logged the results and reported the Tensorboard visualizations of the  models with lowest chi squared/ndf 

## My task
I optimized the following hyperparameters:
- Generator neural network nodes (width and depth)
- Batch size
- Discriminator/Generator ratio
- Learning rate
- Gradient Penalty (to mitigate overfitting)

## Screenshots of the project

+ These are chi squared results with the red graphs representing normalized energy distribution with hits on yaxis from Geant4 full simlation and the black graphs representingthat from BoloGAN.
+ The final chi squared is average of all the chi square values in the subplots of all 15 energy levels
  
Result of the model at the start of the oroject
![Best-reducedchi2-Pions](https://github.com/user-attachments/assets/cc81a3ae-6c27-48f7-b9f9-a07359e1f4dd)

Final result of the model after the project (Improvement: 28%)
<img width="448" alt="Best-BigModel" src="https://github.com/user-attachments/assets/6ca5e76f-3342-41d1-aec4-ff2f8fd65742" />





# BoloGANtainer

Container to deploy BoloGAN GAN training onto other clusters than usual ones.

## Ingredients

### On This Repo

- BoloGAN: contains the GAN training code;
- BoloGANtainer.def: the container recipe file.

### Not on This Repo

- BoloGANtainer.sif: the container itself;
- data: the folder with the training data.

Both are very heavy. We'll let you have them in another way.

## Basics

**BoloGAN** is a Machine Learning system based on GANs (Generative Adversarial Networks) to run fast simulation of the ATLAS calorimeter. After its GANs have been trained, they are used to run simulation with more speed and less resource request than with "traditional" simulation (properly *full simulation*, done with the *Geant4* programme) but preserving good accuracy. The computationally-intensive part of BoloGAN is GAN training itself.

**BoloGANtainer** is a *container* to deploy BoloGAN GAN training, having BoloGAN been built with CERN computing systems in mind, onto other systems. In this way, other resources than CERN own ones can be used for this task, further reducing resource footprint.

## Structure of BoloGAN

You find all BoloGAN content inside the ```BoloGAN``` folder. For GAN training, you must first run the training phase proper and then the evaluation phase, where the best GAN training checkpoint is chosen. The important scripts are the ones below:

- ```common/config.sh```: you find here the general variables, i.e. the ones used by all BoloGAN scripts (the only exception is variable ```vox_dir```, defining the folder where output and processing data are stored during training and evaluation of the best training checkpoint, which is instead at the beginning of ```common/functions.sh```);
- ```model/launch_V2_seed_local.sh```: it runs GAN training. GANs are trained for 1M epochs by default and one checkpoint is saved every 1000 epochs;
- ```plotting/find_best_checkpoints_seed_local.sh```: it runs evaluation of each GAN training checkpoint and chooses the best one. It compares samples generated by each checkpoint with samples generated by the full simulation programme: the checkpoint yielding the lowest reduced chisquare is the one which produced the most similar results to the full simulation programme and thus the best one. It also produces plots showing this comparison.

Each of the scripts above also launches other scripts, each looking after a specific sub-task. Then there are many others which deal with functions of the programme not of our interest (e.g. the voxelisation phase: the programme can also start from raw data and voxelise them, but we'll only deal with already voxelised data) or are legacy options.

## How to run

We use the Apptainer container system to provide an easy way to run the project.

### 1. Building the container

To build the container, run the following command:

```shell
apptainer build ./BoloGANtainer.sif BoloGANtainer.def
```

### 2. Running the container

We need to export three environment variables to correctly run the container:

- `DATADIR`: the directory containing the training data;
- `WORKINGDIR`: the directory meant to host output and processing files;
- `CODEDIR`: the directory containing the BoloGAN code.

So first, create a directory meant to host output and processing files and export it as `WORKINGDIR`.

```shell
mkdir <path_to_workingdir>
export WORKINGDIR=<path_to_workingdir>
```

Then, export the directory containing the training data as `DATADIR`. More on this later ([Data](#data)).

```shell
export DATADIR=<path_to_data>
```

Finally, export the directory containing the BoloGAN code as `CODEDIR`.

```shell
export CODEDIR=<path_to_BoloGAN>
```

### 2.1 Training

```shell
apptainer run -B $WORKINGDIR -B $DATADIR -B $CODEDIR <path_to_BoloGANtainer.sif> train [pions/photons]
```

### 2.2 Evaluation

```shell
apptainer run -B $WORKINGDIR -B $DATADIR -B $CODEDIR <path_to_BoloGANtainer.sif> bestiter [pions/photons]
```

The command above will run the evaluation of the best iteration for pions or photons.

## Miscellanea

If you want to open a shell inside the container run

```shell
apptainer shell -B $WORKINGDIR -B $DATADIR  -B $CODEDIR <path_to_BoloGANtainer.sif>
```

**IMPORTANT: on some clusters Apptainer still bears its old name of Singularity. If this is the case replace ```apptainer``` with ```singularity``` in all commands.**

**IMPORTANT #2: some clusters deny Apptainer containers permission to access files even though correctly bind-mounted and with proper permissions, preventing the programme from running. If this is the case, add the ```-u``` flag to all Apptainer commands. If this also fails, use the ```--fakeroot``` flag instead.**

Instructions will follow on how to run with GPUs enabled.

## Data

The data is provided by the ATLAS collaboration and is not available on this repository. It has a structure like the following:

```
data
├── csvFiles
├── rootFiles
└── binning.xml
```

- The `csvFiles` directory contains the CSV files with the training data.
- The `rootFiles` directory contains the ROOT files with the training data.
- The `binning.xml` file contains the binning information.










 






